[{"content":"I\u0026rsquo;ve set out to write a compiler for Compost, the experimental programming language I came up with about a month ago. The repository for it can be found on GitHub and there is also a playground available where you can try out the language. This is the first part of a series of blogs detailing my experience of writing my first compiler.\nStructure of a Compiler Before embarking on this journey, I did some research on how compilers normally work. Almost all sources described these phases that a compiler goes through:\nLexical Analysis or Tokenization Syntactic Analysis or Parsing Semantic Analysis Intermediate Code Generation Code Optimization Binary Code Generation So far, I\u0026rsquo;ve implemented the first three steps in the compiler, and added a different fourth step: Running the Code and Displaying the Result.\nBecause the Compost language currently doesn\u0026rsquo;t support any type of input, the result of a program will always be the same. In a sense, each program can be optimized into a single constant, which is then output to the console. For this reason, it doesn\u0026rsquo;t make a lot of sense to compile into a binary at this point. The compiler just outputs the constant result for now.\nHowever, I am planning to add input methods to the language in the future, which will make it worth to implement the remaining three steps of compilation. For the last two steps I\u0026rsquo;m planning to use LLVM, which will hopefully make that part a lot less painful.\nLexical Analysis In this post I will focus on how I implemented the first phase of compilation in Rust. Lexical Analysis, or Tokenization, is the process of grouping characters of the code into a sequence of \u0026rsquo;tokens\u0026rsquo;, which are the smallest meaningful units code can be broken up into. It will also throw meaningful errors when there is an unexpected character.\nRust\u0026rsquo;s enums are ideal for representing tokens. This is the Token enum of my compiler as of writing this article:\npub enum Token { Down(Level), Up(Level), Next(Next), Eof, Kw(Kw), Local(String), Global(String), Op(Op), Lit(Lit), Space, } The types Level, Next, Kw (Keyword), Op (Operator) and Lit (Literal) are also Rust enums. Example:\npub enum Kw { Mod, Class, Struct, Traits, Defs, Lets, Zelf, } To read tokens from a string, I\u0026rsquo;ve written the next_token functions. It returns the next token, and the amount of characters that token spans.\ntype SizedToken = (Option\u0026lt;Token\u0026gt;, usize); pub fn next_token(code: \u0026amp;str) -\u0026gt; Result\u0026lt;SizedToken, ErrorMessage\u0026gt; { let char = match code.chars().next() { Some(c) =\u0026gt; c, None =\u0026gt; return Ok((Some(Token::Eof), 1)), }; let token = match char { \u0026#39; \u0026#39; =\u0026gt; (Some(Token::Space), 1), \u0026#39;#\u0026#39; =\u0026gt; (None, comment_size(code)), \u0026#39;(\u0026#39; =\u0026gt; (Some(Token::Down(Level::Paren)), 1), \u0026#39;)\u0026#39; =\u0026gt; (Some(Token::Up(Level::Paren)), 1), \u0026#39;:\u0026#39; =\u0026gt; (Some(Token::Down(Level::Colon)), 1), \u0026#39;\\n\u0026#39; | \u0026#39;\\r\u0026#39; =\u0026gt; (Some(Token::Next(Next::Line)), 1), \u0026#39;,\u0026#39; =\u0026gt; (Some(Token::Next(Next::Comma)), 1), \u0026#39;+\u0026#39; =\u0026gt; (Some(Token::Op(Op::Add)), 1), // ... \u0026#39;a\u0026#39;..=\u0026#39;z\u0026#39; =\u0026gt; next_local_token(code), \u0026#39;A\u0026#39;..=\u0026#39;Z\u0026#39; | \u0026#39;\\\\\u0026#39; =\u0026gt; next_global_token(code), \u0026#39;0\u0026#39;..=\u0026#39;9\u0026#39; =\u0026gt; next_number_token(code), \u0026#39;\\\u0026#39;\u0026#39; =\u0026gt; next_string_token(code), _ =\u0026gt; return Err(ErrorMessage::UnexpectedChar(char.to_string())), }; Ok(token) } Based on the first character, it either returns a simple token with size 1, or it uses other functions to continue to read more complex tokens. ErrorMessage is another enum which is used for errors across the compiler.\nAll of the code above can be found here.\nLevels in Compost Down, Up and Next are features of Compost\u0026rsquo;s syntax. Compost uses levels to organise its code. Look at this Compost code:\nmod ModuleA class x Int y Int mod ModuleB class x: Int y: Int mod ModuleC class(x: Int, y: Int) All three modules are semantically the same. The only difference is the levels syntax they use.\nThe three ways of organising code into levels are:\nIndentation. The more indented something is, the deeper the level. Using colons (:). Code on the same line after the colon is on a deeper level. After a comma (,) the code is on the original level again. Using parentheses (( and )). Code within parentheses is on a deeper level. During tokenization, each Token is assigned a level based on these three syntaxes. The type for a Token with a level is a simple tuple:\npub type LeveledToken = (Token, usize); To keep track of the level while tokenizing, I\u0026rsquo;ve created a utility struct called LevelStack.\n/// The type of level syntax. pub enum Level { Colon, Paren, } /// Used to separate levels. pub enum Next { Comma, Line, } /// Utility to keep track of the depth level of our code. struct LevelStack { // The current stack of Colon and Parentheses levels. levels: Vec\u0026lt;Level\u0026gt;, // The current indentation level. indentation: usize, } impl LevelStack { fn new() -\u0026gt; Self { LevelStack { levels: vec![], indentation: 0, } } /// Go deeper. fn push(\u0026amp;mut self, level: Level) { self.levels.push(level) } /// Go up to a specific type of Level. fn pop(\u0026amp;mut self, level: \u0026amp;Level) { if let Some(popped_level) = self.levels.pop() { match level { Level::Paren =\u0026gt; { // Keep popping until we\u0026#39;re at an opening parenthesis. if popped_level != Level::Paren { self.pop(level) } } Level::Colon =\u0026gt; { // If the last level wasn\u0026#39;t a colon, push it back. if popped_level != Level::Colon { self.push(popped_level) } } } } } /// Add one indentation level. fn indent(\u0026amp;mut self) { self.indentation += 1 } /// Process a \u0026#39;Next\u0026#39; token. fn next(\u0026amp;mut self, next: \u0026amp;Next) { match next { Next::Line =\u0026gt; { // Clear all colon levels. self.levels.retain(|level| level != \u0026amp;Level::Colon); // Reset indentation. self.indentation = 0; } // Go back to the latest Colon level when encountering a comma. Next::Comma =\u0026gt; self.pop(\u0026amp;Level::Colon), } } /// Gets the current level. fn level(\u0026amp;self) -\u0026gt; usize { self.levels.len() + self.indentation } } The full code can be found here.\nThe Tokenize Function All of the functionality described above comes together in the tokenize function. It takes a string of raw code and returns the leveled tokens.\npub fn tokenize(code: \u0026amp;str) -\u0026gt; Tokens { let mut position: usize = 0; let mut level_stack = LevelStack::new(); let mut leveled_tokens: Vec\u0026lt;LeveledToken\u0026gt; = vec![]; let mut is_beginning_of_line = true; while position \u0026lt;= code.len() { // Use the next_token function to get the next token. let sized_token = match next_token(\u0026amp;code[position..]) { Ok(sized_token) =\u0026gt; sized_token, Err(message) =\u0026gt; // ... pass on the error }; // Move forward the position we are at in the code. position += sized_token.1; if let Some(token) = sized_token.0 { // Check if we are still at the beginning of a line. // Spaces at the beginning are not regarded as \u0026#39;the line\u0026#39;, but as markers for indentation. is_beginning_of_line = is_beginning_of_line \u0026amp;\u0026amp; token == Token::Space; match token { // Add indentation if we have another space at the beginning of the line. Token::Space =\u0026gt; if is_beginning_of_line { level_stack.indent() } // Push or pop levels according to the received tokens. Token::Down(level) =\u0026gt; level_stack.push(level), Token::Up(level) =\u0026gt; level_stack.pop(\u0026amp;level), Token::Next(next) =\u0026gt; { level_stack.next(\u0026amp;next); if next == Next::Line { is_beginning_of_line = true } } Token::Eof =\u0026gt; leveled_tokens.push((Token::Eof, 0)), // Any other tokens are regular tokens without anything to do with our levels. // We simply add them to our vector of tokens at the current level. _ =\u0026gt; leveled_tokens.push((token, level_stack.level())), } } } leveled_tokens.into() } This code is available here.\nThe \u0026lsquo;Tokens\u0026rsquo; Utility Struct As you might have noticed, the output type of the tokenize function is Tokens. This is another utility struct which facilitates traversing the tokens in the next phase: Syntactic Analysis.\n/// Provides utility functions that help traversing the tokens. pub struct Tokens { tokens: Vec\u0026lt;LeveledToken\u0026gt;, position: usize, } impl Tokens { /// Advance to the next token. pub fn step(\u0026amp;mut self) { self.position += 1; } /// Whether there are more tokens left. pub fn still_more(\u0026amp;self) -\u0026gt; bool { self.position \u0026lt; self.tokens.len() } /// Whether the current token is deeper than the given level. pub fn deeper_than(\u0026amp;self, level: usize) -\u0026gt; bool { self.still_more() \u0026amp;\u0026amp; self.level() \u0026gt; level } pub fn deeper_than_or_eq(\u0026amp;self, level: usize) -\u0026gt; bool { self.still_more() \u0026amp;\u0026amp; self.level() \u0026gt;= level } /// The remaining tokens. pub fn remaining(\u0026amp;self) -\u0026gt; \u0026amp;[LeveledToken] { \u0026amp;self.tokens[self.position..] } /// The current token. pub fn token(\u0026amp;self) -\u0026gt; \u0026amp;Token { \u0026amp;self.tokens[self.position].0 } /// The current level. pub fn level(\u0026amp;self) -\u0026gt; usize { self.tokens[self.position].1 } } The source of this struct is in this file.\nConclusion Tokenization is probably the most straightforward part of compilation. By linearly traversing the characters code we get a vector of tokens. Rust\u0026rsquo;s enum type is ideal for representing tokens. Using enums will come in handy at later phases because we can use match statements to decide what happens when a token is encountered.\nThe most complex part of this phase was adding \u0026rsquo;levels\u0026rsquo;, which is a unique feature of the Compost language that provides multiple syntaxes for organising the code into levels.\nIf you are curious to see how I turn these tokens into an abstract syntax tree, read Part 2: Syntactic Analysis.\n","permalink":"https://sytzez.com/blog/creating-a-compiler-for-compost-using-rust-part-1-lexical-analysis/","summary":"I\u0026rsquo;ve set out to write a compiler for Compost, the experimental programming language I came up with about a month ago. The repository for it can be found on GitHub and there is also a playground available where you can try out the language. This is the first part of a series of blogs detailing my experience of writing my first compiler.\nStructure of a Compiler Before embarking on this journey, I did some research on how compilers normally work.","title":"Creating a Compiler for Compost using Rust, Part 1: Lexical Analysis"},{"content":"In the previous post, I described how I implemented lexical analysis for Compost. The next phase in compilation is syntactic analysis, which turns the string of tokens into an \u0026lsquo;abstract syntax tree\u0026rsquo;. It also throws meaningful errors when wrong syntax is used.\nThe Abstract Syntax Tree An abstract syntax tree (AST) represents the code at the level of statements and expressions. Every type of statement and expression has its own data structure, which can contain other statements or expressions, forming a tree of data structures.\nThe AST does not yet make meaningful links between different statements, this is left to the next stage of semantic analysis. Instead, it just checks that the syntax of each individual statement and expression is correct.\nStatements The tree of statements of the Compost programming language looks like this:\ngraph TD AST[AST Root] L[Lets] M[Modules] C[\"Class/Struct\"] F[Fields] T[Traits] D[Defs] L2[Lets] AST--\u003eL AST--\u003eM M--\u003eC C--\u003eF M--\u003eT M--\u003eD M--\u003eL2 As you can see, it\u0026rsquo;s not too complicated! A program has lets (functions and constants) and modules A module can have a class or a struct, or neither. A module can also have traits, defs (definitions of traits for that module), and lets (more functions and constants). I might implement sub-modules in the future, but for now that hasn\u0026rsquo;t been necessary.\nExpressions Now let\u0026rsquo;s look at expressions. Each let and def has an expression. Expressions represent operations performed on the data of the program to form a new value. To give you an idea of what expressions can be: 1 + 1, .X + .Y and Function(a: ConstA, b: ConstB) are all expressions. In our Rust code, an Expression is defined as an enum:\npub enum Expression { // e.g. ... + ... Binary(BinaryCall), // e.g. MyFunction(a: ..., b: ...) Let(LetCall), // e.g. MyInstance.MyTrait(a: ..., b: ...) Def(DefCall), // e.g. 42 Literal(RawValue), // e.g. myField Local(String), // e.g. myField.theirField FriendlyField(FriendlyField), // Self Zelf, } pub struct BinaryCall { // The operator. pub op: BinaryOp, // The left side of the operation, another expression. pub lhs: Box\u0026lt;Expression\u0026gt;, // The right side of the operation, another expression. pub rhs: Box\u0026lt;Expression\u0026gt;, } pub struct LetCall { // The name of the let being called. pub name: String, // The list of inputs, and their expressions. pub inputs: HashMap\u0026lt;String, Expression\u0026gt;, } pub struct DefCall { // The name of the trait being called. pub name: String, // The expression before the dot. pub subject: Box\u0026lt;Expression\u0026gt;, // The list of inputs and their expressions. pub inputs: HashMap\u0026lt;String, Expression\u0026gt;, } pub struct FriendlyField { pub local_name: String, pub field_name: String, } The full code can be found here.\nTo demonstrate, the tree of the expression Self.MyTrait(x: MyConst + 10) will look like this:\ngraph TD DC[\"DefCall (MyTrait)\"] S[Self] BC[\"BinaryCall (+)\"] LC[\"LetCall (MyConst)\"] L[\"Literal (10)\"] DC-- Subject --\u003eS DC-- \"Parameter: x\" --\u003eBC BC-- lhs --\u003eLC BC-- rhs --\u003eL Types Besides statements and expressions, we also have syntax for types. Types are due some improvement, so I\u0026rsquo;ll skip the discussion for now. Currently a type can only be Self, the name of a trait or the name of a module.\nThe \u0026lsquo;Parser\u0026rsquo; Trait Rust\u0026rsquo;s traits provide a way to have uniform interfaces across different types. This comes in handy for structuring our parsing logic.\nI created a Parser trait that can be implemented on any type that can be parsed from Tokens. It features a matches method, which checks if the upcoming token(s) match the type of statement that is being parsed, and it features a parse method which takes the tokens and returns the parsed statement or an error if the syntax is incorrect.\nThe trait also provides a maybe_parse method which does both. It checks if the tokens match the statement and parses the statement if it does, or does nothing otherwise. The maybe_parse method comes in handy when different types of statement can be expected.\npub trait Parser where Self: Sized, { /// Whether the upcoming tokens match this type of parser. fn matches(tokens: \u0026amp;[LeveledToken]) -\u0026gt; bool; /// Parse the tokens into a statement for the abstract syntax tree. fn parse(tokens: \u0026amp;mut Tokens) -\u0026gt; CResult\u0026lt;Self\u0026gt;; /// Parse tokens if they match this parser, otherwise do nothing. fn maybe_parse(tokens: \u0026amp;mut Tokens) -\u0026gt; CResult\u0026lt;Option\u0026lt;Self\u0026gt;\u0026gt; { if Self::matches(tokens.remaining()) { Ok(Some(Self::parse(tokens)?)) } else { Ok(None) } } } Full source here.\nUsing the maybe_parse method, we can parse multiple possibile statements using Rust\u0026rsquo;s if let and else if let statements. See how it\u0026rsquo;s used inside the code for parsing a mod statement. Inside a mod we can expect class, struct, traits or defs statements.\n/// The AST node for a module statement, which starts with \u0026#39;mod\u0026#39;. pub struct ModuleStatement { pub name: String, pub class: Option\u0026lt;ClassStatement\u0026gt;, pub strukt: Option\u0026lt;StructStatement\u0026gt;, pub traits: Vec\u0026lt;TraitStatement\u0026gt;, pub defs: Vec\u0026lt;DefStatement\u0026gt;, pub lets: Vec\u0026lt;LetStatement\u0026gt;, } impl Parser for ModuleStatement { /// If the next token is the keyword \u0026#39;mod\u0026#39;, we can expect this type of statement. fn matches(tokens: \u0026amp;[LeveledToken]) -\u0026gt; bool { matches!(tokens[0].0, Token::Kw(Kw::Mod)) } /// The code that parses the next few tokens into a module statement. fn parse(tokens: \u0026amp;mut Tokens) -\u0026gt; CResult\u0026lt;Self\u0026gt; { // Check what the base level is, so we know when to exit the statement. let base_level = tokens.level(); // Skip over the \u0026#39;mod\u0026#39; keyword to the next token. tokens.step(); // Parse the name of the module. let name = parse_global(tokens)?; // Create our AST node, to be filled with information from the coming tokens. let mut statement = ModuleStatement::new(name); // Keep looping for as long we\u0026#39;re within the level of the module. while tokens.deeper_than(base_level) { // For each type of expected statement, check if the parser recognises it // and add the parsed statement to our module statement. if let Some(class) = ClassStatement::maybe_parse(tokens)? { statement.class = Some(class); } else if let Some(strukt) = StructStatement::maybe_parse(tokens)? { statement.strukt = Some(strukt); } else if let Some(mut traits) = TraitsStatement::maybe_parse(tokens)? { statement.traits.append(\u0026amp;mut traits.traits); } else if let Some(mut defs) = DefsStatement::maybe_parse(tokens)? { statement.defs.append(\u0026amp;mut defs.defs); } else if let Some(mut lets) = LetsStatement::maybe_parse(tokens)? { statement.lets.append(\u0026amp;mut lets.lets); } else { // Return an error if none of the expected statements match. return tokens.unexpected_token_error(); } } Ok(statement) } } As you can see, using our Parser trait and the Tokens struct from the previous chapter, the code for parsing a statement becomes very clear and structured! Before I did the Parser and Tokens refactors, this code was much lengthier and uglier. See the full code here.\nParsing Expressions Parsing expressions is a bit harder, because there are so many possible types of expressions. You\u0026rsquo;ve seen the possibilities in the chapter about Expressions. Luckily we can use match statements on our Token enum, to deal with every possible incoming token.\nimpl Parser for Expression { /// We can\u0026#39;t really predict if a series of tokens is going to be an expression, /// it has to be determined from the context. fn matches(_tokens: \u0026amp;[LeveledToken]) -\u0026gt; bool { true } fn parse(tokens: \u0026amp;mut Tokens) -\u0026gt; CResult\u0026lt;Self\u0026gt; { // Check what the base level is for this expression. let base_level = tokens.level(); // The first token needs cloning to prevent immutable borrow errors. let token = tokens.token().clone(); // We parse the first token differently because there are different possibilities // than any following tokens. For example, a `.` at the beginning implies a trait // call on `Self`, while a `.` further on just means a trait call on whatever came // before it. let mut expr = match token { // The \u0026#39;Self\u0026#39; keyword. Token::Kw(Kw::Zelf) =\u0026gt; { // Step to the next token. tokens.step(); Expression::Zelf } // ... more possibilities omitted here // The . operator at the beginning. Implies an operation on \u0026#39;Self\u0026#39;. Token::Op(Op::Dot) =\u0026gt; { // We don\u0026#39;t step to the next token, so we can reevaluate the same dot in the next step. Expression::Zelf } // The - operator. Calls the Op\\Neg trait on whatever comes after. Token::Op(Op::Sub) =\u0026gt; { tokens.step(); let expr = Expression::parse(tokens)?; Expression::Def(DefCall { name: \u0026#34;Op\\\\Neg\u0026#34;.to_string(), subject: Box::new(expr), inputs: [].into(), }) } // Any other tokens cause a compiler error. _ =\u0026gt; return tokens.unexpected_token_error(), }; // Parse further operations, for as long as the tokens are on the level of this expression. while tokens.deeper_than_or_eq(base_level) { // Again, the token needs cloning to prevent immutable borrow errors. let token = tokens.token().clone(); // Check what the next token is. expr = match token { Token::Op(op) =\u0026gt; { match op { // Logic for dealing with binary operators. Op::Add | Op::Sub | Op::Mul | Op::Div =\u0026gt; { tokens.step(); // Parse the tokens which come after it into an expression for the right hand side. let rhs = Expression::parse(tokens)?; Expression::Binary(BinaryCall { op: match op { Op::Add =\u0026gt; BinaryOp::Add, Op::Sub =\u0026gt; BinaryOp::Sub, Op::Mul =\u0026gt; BinaryOp::Mul, Op::Div =\u0026gt; BinaryOp::Div, _ =\u0026gt; unreachable!(), }, // Put the expression as parsed so far into the left hand side. lhs: Box::new(expr), // Put the expression which comes afterwards into the right hand side rhs: Box::new(rhs), }) } Op::Dot =\u0026gt; { // Omitted... Logic for dealing with Trait calls or FriendlyFields. } _ =\u0026gt; return tokens.unexpected_token_error(), } } _ =\u0026gt; return tokens.unexpected_token_error(), } } Ok(expr) } } See the full file here.\nConclusion Using the Parser trait, we\u0026rsquo;ve been able to parse tokens into an AST in a clean and structured manner. Each type of statement has its own struct and its own implementation of Parser which parse the tokens into a node of the AST.\nWith a full AST, we have a complete representation of all statements and expressions in the code. However, the names in the code don\u0026rsquo;t mean anything yet. Whenever there is a call to a let, it\u0026rsquo;s not connected to the definition of that let yet.\nThis \u0026lsquo;meaning\u0026rsquo; of the code is analysed in the next step called semantic analysis. You can read about it in the next part of this series.\n","permalink":"https://sytzez.com/blog/creating-a-compiler-for-compost-using-rust-part-2-syntactic-analysis/","summary":"In the previous post, I described how I implemented lexical analysis for Compost. The next phase in compilation is syntactic analysis, which turns the string of tokens into an \u0026lsquo;abstract syntax tree\u0026rsquo;. It also throws meaningful errors when wrong syntax is used.\nThe Abstract Syntax Tree An abstract syntax tree (AST) represents the code at the level of statements and expressions. Every type of statement and expression has its own data structure, which can contain other statements or expressions, forming a tree of data structures.","title":"Creating a Compiler for Compost using Rust, Part 2: Syntactic Analysis"},{"content":"In the previous installment of this series, I described how I turned the tokens into an abstract syntax tree (AST). The AST contains all statements, expressions and types of the program, but doesn\u0026rsquo;t link the together. All the names of variables, modules, traits and functions are simply Rust Strings without any meaning beyond that.\nTo give meaning to these names we can use semantic analysis, which resolves the names into references to the right piece of information in our program. It can also give some more elaborate error messages if anything in the code doesn\u0026rsquo;t link up correctly.\nSemantic Analysis Semantic analysis, also known as context sensitive analysis, does many things. In the Compost compiler, this has been the most difficult phase to work out so far.\nFirstly, it generates symbol tables from the code, which store \u0026lsquo;symbols\u0026rsquo;, the names of things, against real information about the program. There are tables for traits, lets, modules, local variables, etc. Each part of the code that can be represented by a name needs to have a symbol table.\nSecondly, it resolves each place where such a symbol is used to the correct information. This can be context dependent. You can, for example, reference something differently from inside a module by leaving out the module prefix. Inside module Op, you can use Add instead of the full Op\\Add.\nFurthermore, it checks that all types match up. It needs to figure out the output type of each expression, and check if it matches the input type of wherever the expression is being used.\nFinally, it transforms all Expression statement enums into what I called Evaluation enums, which have more complete information. They contain references to the actual functions, constants, traits, structs and classes being referenced, rather than just their names in the form of a String. In the Evaluation form they are almost ready to be evaluated into a result.\nSymbol Tables I created the Table struct to contain all the logic for declaring and resolving symbols. It splits symbols up by the \\ character into a vector of smaller strings, called a path.\nWhen an item is declared, it is added to the items vector in the form of a tuple with the path on one side and a Rc (reference counted pointer) to the item on the right side. Rust\u0026rsquo;s Rc pointer was a good choice here, because items can be retrieved from the table in many places in the code which can then also pass them on to other places. Using a regular \u0026amp; reference here would have caused Rust hell.\npub struct Table\u0026lt;T\u0026gt; { // The name of the table, just used for error messages. name: \u0026amp;\u0026#39;static str, // The items inside the table, with the path on the left. items: Vec\u0026lt;(Vec\u0026lt;String\u0026gt;, Rc\u0026lt;T\u0026gt;)\u0026gt;, // The longest path length in the table. Used for the algorithm that resolves symbols. longest_path: usize, } The function that resolves a symbol loops through the items to find a match. First, it tries the shortest possible path in the table that matches the given path. If that fails, it will try longer and longer paths.\nThe function also takes into account the scope from where the table was accessed. First it will try to search within the scope to find a resolution. If that fails, it will try outside of the scope.\n/// Resolves the best match of the given path. /// When \u0026#34;Add\u0026#34; and \u0026#34;Op\\Add\u0026#34; are available, \u0026#34;Add\u0026#34; should resolve to the former. /// In the \u0026#34;Op\u0026#34; scope, \u0026#34;Add\u0026#34; should resolve to the latter. /// When only \u0026#34;Op\\Add\u0026#34; is available, \u0026#34;Add\u0026#34; should resolve to that. /// Only when the name is not available inside the scope, global search should be tried. pub fn resolve(\u0026amp;self, name: \u0026amp;str, scope: \u0026amp;str) -\u0026gt; CResult\u0026lt;Rc\u0026lt;T\u0026gt;\u0026gt; { // Form the path by combining the scope and the symbol name. // \u0026#34;Add\u0026#34; in scope \u0026#34;Op\u0026#34; will become \u0026#34;Op/Add\u0026#34;. let path = [Self::path(scope), Self::path(name)].concat(); // Check shortest paths that match first, then longer ones. for match_len in path.len()..=self.longest_path { for (item_path, item) in self.items.iter() { if item_path.len() == match_len { // Check if the shortened end of the path matches the given path. let start = item_path.len() - path.len(); let shortened_item_path = \u0026amp;item_path[start..]; // Return a Rc to the item on a match. if shortened_item_path == path { return Ok(Rc::clone(item)); } } } } if !scope.is_empty() { // If nothing was found, and the scope is not empty, // try searching everywhere by omitting the scope. self.resolve(name, \u0026#34;\u0026#34;) } else { // If nothing was found even globally, return an error. error(ErrorMessage::NoResolution(self.name, name.into())) } } The full code for Table can be found here.\nAnalysing the AST The analyse_ast function analyses the whole AST and returns a SemanticContext, which contains all accessible elements of the code within symbol tables.\npub struct SemanticContext { pub traits: Table\u0026lt;RefCell\u0026lt;Trait\u0026gt;\u0026gt;, pub lets: Table\u0026lt;RefCell\u0026lt;Let\u0026gt;\u0026gt;, pub interfaces: Table\u0026lt;RefCell\u0026lt;Interface\u0026gt;\u0026gt;, } While the SemanticContext looks fairly simple. The process to get that result incorporates many steps of analysis over the AST. All items are wrapped into Rust\u0026rsquo;s RefCell, to allow them to be changed at different steps of analysis.\nStep 1: Populating Trait and Interface Identifiers. The first step is simply populating all traits and interface identifiers that exist in the program. An interface is a type that\u0026rsquo;s automatically defined for each module, it\u0026rsquo;s simply a combination of all the traits in that module. Populating the traits and interfaces simply adds records to the right tables with dummy items, to be replaced later:\n// Loop through all modules in the AST. for module in ast.mods.iter() { // Add the identifier for this module\u0026#39;s interface. let dummy_interface = context.interfaces.declare(\u0026amp;module.name, RefCell::new(vec![]))?; // Loop through all trait statements in this module. for trait_statement in module.traits.iter() { let name = format!(\u0026#34;{}\\\\{}\u0026#34;, module.name, trait_statement.name); // Add the identifier for this trait. The trait is connected to the interface of the module, which is needed for the next step. context .traits .declare(\u0026amp;name, RefCell::new(Trait::dummy(\u0026amp;name, \u0026amp;dummy_interface)))?; } // Each module has an eponymous trait. This trait can be used to convert other instances into something of that module\u0026#39;s type. // For example, the String module creates a String trait, which can be defined on other modules to turn those into a String. context .traits .declare(\u0026amp;module.name, RefCell::new(Trait::dummy(\u0026amp;module.name, \u0026amp;dummy_interface)))?; } Afterwards, there is a slightly complex process to expand the traits contained in each interface. Because Compost has a feature calles \u0026lsquo;automatic definitions\u0026rsquo;, some traits are automatically defined on modules which they don\u0026rsquo;t appear on in the code. This process adds those traits to the interface types of those modules. Sometimes, the process needs to be repeated a few times before everything is included.\n// Fill module interfaces, made up of the module\u0026#39;s own traits and def traits from other modules. // By this point, all trait identifiers have been populated. for module in ast.mods.iter() { let mut interface = vec![]; // The module\u0026#39;s own traits. for trait_statement in module.traits.iter() { let trayt = context .traits .resolve(\u0026amp;trait_statement.name, \u0026amp;module.name)?; interface.push(trayt); } // Traits added on from other modules through defs. for def in module.defs.iter() { let trayt = context.traits.resolve(\u0026amp;def.name, \u0026amp;module.name)?; interface.push(trayt); } let output = interface_type(\u0026amp;interface); context.interfaces.resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)?.replace(interface); let eponymous_trait = Trait { full_name: module.name.clone(), interface: context.interfaces.resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)?, inputs: vec![], output, default_definition: None, }; context .traits .resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)? .replace(eponymous_trait); } // Add automatic definitions to interfaces. Repeat until stable. loop { let mut added_num_of_traits: usize = 0; for module in ast.mods.iter() { let own_interface = context.interfaces.resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)?; let mut related_interfaces = vec![]; for def in module.defs.iter() { let trayt = context.traits.resolve(\u0026amp;def.name, \u0026amp;module.name)?; related_interfaces.push(Rc::clone(\u0026amp;trayt.borrow().interface)); } for related_interface in related_interfaces.iter() { for trayt in related_interface.borrow().iter() { if own_interface.borrow().iter().any(|t| t == trayt) { continue; } own_interface.replace_with(|old| { old.push(Rc::clone(trayt)); old.clone() }); added_num_of_traits += 1 } } } if added_num_of_traits == 0 { break } } I must admit this code is not very clear. It can probably be refactored.\nStep 2: Analysing Input and Output Types By this point, all traits and interfaces have been populated. Since Compost types are composed of traits and interfaces, it\u0026rsquo;s now possible to resolve all types in the program. Types occur in the inputs and outputs of traits, lets and defs. Lets include the constructors of structs and classes.\n// Loop over all global lets, and analyse their types. for let_statement in ast.lets.iter() { let lett = Let::analyse_just_types(let_statement, \u0026amp;context, \u0026#34;\u0026#34;)?; context .lets .declare(\u0026amp;let_statement.name, RefCell::new(lett))?; } // Loop over all modules. for module in ast.mods.iter() { // Analyse all trait input and output types, and analyse their default definitions. for trait_statement in module.traits.iter() { let trayt = Trait::analyse(trait_statement, module, \u0026amp;context, false)?; context .traits .resolve(\u0026amp;trait_statement.name, \u0026amp;module.name)? .replace(trayt); } // Populate module let identifiers, analyse their types. for let_statement in module.lets.iter() { let name = format!(\u0026#34;{}\\\\{}\u0026#34;, module.name, let_statement.name); let lett = Let::analyse_just_types(let_statement, \u0026amp;context, \u0026amp;module.name)?; context.lets.declare(\u0026amp;name, RefCell::new(lett))?; } // Populate struct and class constructors and definition identifiers, and analyse their types. if let Some(struct_statement) = \u0026amp;module.strukt { // Just the inputs and output of the constructor. let constructor = Let { inputs: Struct::constructor_inputs(struct_statement), output: interface_type(context.interfaces.resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)?.borrow().as_ref()), evaluation: Evaluation::Zelf, }; context .lets .declare(\u0026amp;module.name, RefCell::new(constructor))?; } else if module.class.is_some() { // Just the inputs and output of the constructor. let constructor = Let { inputs: Class::constructor_inputs(module, \u0026amp;context)?, output: interface_type(context.interfaces.resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)?.borrow().as_ref()), evaluation: Evaluation::Zelf, }; context .lets .declare(\u0026amp;module.name, RefCell::new(constructor))?; } } Step 3: Analyse Expressions Now that we have all types, as well as identifiers for all lets, traits and defs, including their input and output types, we can finally analyse the expressions occurring throughout the program. Expressions are part of let and def statements.\n// Analyse global let expressions. for let_statement in ast.lets.iter() { let lett = Let::analyse(let_statement, \u0026amp;context, \u0026#34;\u0026#34;)?; context.lets.resolve(\u0026amp;let_statement.name, \u0026#34;\u0026#34;)?.replace(lett); } for module in ast.mods.iter() { // Analyse module let expressions. for let_statement in module.lets.iter() { let lett = Let::analyse(let_statement, \u0026amp;context, \u0026amp;module.name)?; context .lets .resolve(\u0026amp;let_statement.name, \u0026amp;module.name)? .replace(lett); } // Re-analyse traits with default definitions. for trait_statement in module.traits.iter() { let trayt = Trait::analyse(trait_statement, module, \u0026amp;context, true)?; context .traits .resolve(\u0026amp;trait_statement.name, \u0026amp;module.name)? .replace(trayt); } // Analyse struct and class constructor and def expressions. if module.strukt.is_some() { let strukt = Struct::analyse(module, \u0026amp;context)?; context .lets .resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)? .replace(strukt.constructor()); } else if module.class.is_some() { let class = Class::analyse(module, \u0026amp;context)?; context .lets .resolve(\u0026amp;module.name, \u0026#34;\u0026#34;)? .replace(class.constructor()); } } Find the full code for analyse_ast here.\nAnalysing Evaluations from Expressions All the *::analyse methods that are used in the analyse_ast function are defined in different places. They analyse Lets, Traits, Modules, Defs, Types and Expressions in detail.\nThe core of these analyses it the Evaluation::analyse method, which take an Expression and analyses everything inside it within the right context, resulting in an Evaluation.\nDuring this analysis, it resolves all the referenced traits and lets, checks that the called traits are callable on the expression they\u0026rsquo;re called on, and verifies that all input and output types are matching1.\nSome parts of this analysis, especially type checking, are still a work of process as of writing this blog. But the whole thing works well enough to make most programs run!\nFind the source code of it here.\nConclusion The most daunting part of the compiler so far has been semantic analysis. It requires many traversions over the AST to resolve and analysis everything in the right order. First we need to know what types (traits and interfaces) are available, then we need to know what lets we have, and what the input and output types of everything is. Then we can analyse the expressions, and check that all types match up.\nThe resulting SemanticContext struct and its contents are so detailed that they can easily be resolved into a result to be displayed in the console, which is what happens in the runtime module. However, ideally I\u0026rsquo;d like to write some more modules that turn this SemanticContext into intermediate code, which can then be compiled into an actual binary by LLVM.\nEither way, even without compiling to a binary, I feel pretty satisfied with this compiler, at the moment more of an interpreter. that can parse Compost code and execute it! If you are interested in the Compost programming language please check out the GitHub repository, which contains more information about the language in the README. And also check out the Compost Playground in which you can run Compost code from your browser.\n","permalink":"https://sytzez.com/blog/creating-a-compiler-for-compost-using-rust-part-3-semantic-analysis/","summary":"In the previous installment of this series, I described how I turned the tokens into an abstract syntax tree (AST). The AST contains all statements, expressions and types of the program, but doesn\u0026rsquo;t link the together. All the names of variables, modules, traits and functions are simply Rust Strings without any meaning beyond that.\nTo give meaning to these names we can use semantic analysis, which resolves the names into references to the right piece of information in our program.","title":"Creating a Compiler for Compost using Rust, Part 3: Semantic Analysis"},{"content":"If you have deadlines on top of people asking you to do stuff ASAP, on top of issues that need to be fixed immediately, on top of many meetings throughout the day, it\u0026rsquo;s easy to lose track of your to do list or prioritise the wrong things.\nMy to do list method has helped me keep an overview of everything I need to do. It\u0026rsquo;s low overhead once you get used to it. It\u0026rsquo;s flexible enough to acommodate unexpected work coming up throughout the day or the week, whilst being rigorous enough to keep track of deadlines and make sure everything I ought to be doing gets done.\nI use an app called Obsidian to manage my notes, but this method will probably work with many other note taking apps. Obsidian uses Markdown for formatting which I find very useful.\nDaily note plugin I use the daily note feature of Obsidian to easily create a fresh note everyday. You can enable it by clicking on the settings icon (looks like a gear) on the bottom left, going to \u0026lsquo;Core plugins\u0026rsquo; and enabling \u0026lsquo;Daily notes\u0026rsquo;. Once enabled, you can edit the settings of the daily note by clicking on the gear next to the plugin name in the same settings tab.\nI recommend having a separate folder just for daily notes, you can specify the folder name in \u0026lsquo;New file location\u0026rsquo;. I call mine \u0026lsquo;Logs\u0026rsquo;, as my notes also function as daily logs of my work. You can provide a template file for your notes as well, which I will get in to later, and you can choose to open the daily note when you open Obsidian. If you choose not to open your daily note on startup, you can press the daily note button on the left of your Obsidian window (it looks like a calendar) to open/create the note.\nThe daily note format My daily notes consist of two bullet lists. At the top, I have the list of tasks I have completed that day. Then I have a heading called \u0026lsquo;TODO\u0026rsquo; followed by another bullet list of all my to do\u0026rsquo;s. Those are not just the to do\u0026rsquo;s of the day, but the longer term ones as well.\nI\u0026rsquo;ll show you an example. Markdown uses - at the beginning of a line for bullet lists and # for headings. You can use [[...]] to link to other notes in your Obsidian vault.\n- Task that I have completed - Another tasks that I have completed - Meeting about xxx. [[Meeting notes]] - A task that I have worked on but not completed # TODO - Task that I have to do urgently - Another urgent task - Task needs to be finished this sprint - Another task for this sprint - 29-10-2022 Task with deadline. - Subtask - Other subtask - 01-2023 Other task with deadline. [[Task details]] Creating the TODO list Every working day, I copy and paste the whole TODO list from the previous daily note to the current one.\nAs I go through emails and messages in the morning, I\u0026rsquo;ll add every request to the top of the list. I\u0026rsquo;ll also add meetings I have that day.\nThen I\u0026rsquo;ll reorder the list to create a rough order of the things I want to do that day. Anything I don\u0026rsquo;t expect to work on that day, I\u0026rsquo;ll leave down lower on the list, separated by an empty line. Anything that needs to be done longer term, say, in more than a weeks time, I\u0026rsquo;ll leave down even lower than that, also separated by an empty line.\nManaging sudden urgent tasks When something urgent comes up during the day, I\u0026rsquo;ll simply add it to the top or near the top of the TODO list. Any tasks that I can\u0026rsquo;t complete that day because of it will just move onto the next day when I copy paste the TODO list.\nManaging deadlines Any task on the TODO list that has a deadline I\u0026rsquo;ll prefix with the date it needs to be completed. That way I see all my deadlines every morning. I\u0026rsquo;ll update the dates on the daily list whenever a deadline changes. As a deadline comes closer, I\u0026rsquo;ll move the task higher up the TODO list.\nThe log of completed work Whenever I complete a task, I\u0026rsquo;ll move the task out of the TODO area into the bullet list at the top of the daily note. I\u0026rsquo;ll call that list the \u0026rsquo;log\u0026rsquo;. If I needed to do anything special or unexpected to complete the task, I add that to the bullet point.\nWhen you do this, you\u0026rsquo;ll end up with logs of what you did every day. This could come in handy if at any point you need to find out what you were doing at a certain time, or when you were doing something, using the search box of Obsidian. You\u0026rsquo;ll have answers if stakeholders ask you why a certain thing couldn\u0026rsquo;t be finished at a certain time.\nIf you need to keep track of hours worked on certain tasks, you can also add the time of completion to each bullet point.\nLonger term tasks If you work on a task but don\u0026rsquo;t complete it that day, just add a bullet point of what you did to the log, without removing the task from the TODO list.\nIf you want, you can also split up tasks in the TODO list by adding bullet points below it with extra indentation.\nLinking to other notes Aside from daily notes, I also use Obsidian to make more detailed notes about other things. For example, I\u0026rsquo;ll create a note for each meeting, and I\u0026rsquo;ll create a note if I\u0026rsquo;m doing research for a certain feature.\nObsidian allows you to link notes to other notes using [[...]] notation:with. So if I have a meeting in the daily note, I\u0026rsquo;ll link it to the meeting notes, and if I have notes related to a certain task, I\u0026rsquo;ll link the bullet point to the task.\nYou can of course also link externally using URL\u0026rsquo;s. For example, you might want to link to a ticket on a planner board.\nOrganising many daily notes Over time, you\u0026rsquo;ll have a long long list of daily notes. You can easily organise them per month. When a new month has begun, simply create a folder with the year and month, e.g. 2022-10, and move all the notes from that period into it. That makes it really easy to look up things you did during a certain time period.\nConclusion This way of organising tasks has helped me stay on top of things, long term and short term, whilst staying flexible enough to react to things that need immediate attention. It has also allowed me to keep a detailed log of everything I have done, and when I have done it. When communicating with stakeholders, I\u0026rsquo;ll know what I was doing at a certain time, and why things might have taken longer. But most of the time that isn\u0026rsquo;t necessary, because I have an overview of everything I need to do and can make the right decisions to get everything done on time.\n","permalink":"https://sytzez.com/blog/how-i-manage-my-work-like-a-boss/","summary":"If you have deadlines on top of people asking you to do stuff ASAP, on top of issues that need to be fixed immediately, on top of many meetings throughout the day, it\u0026rsquo;s easy to lose track of your to do list or prioritise the wrong things.\nMy to do list method has helped me keep an overview of everything I need to do. It\u0026rsquo;s low overhead once you get used to it.","title":"How I manage my work like a boss"},{"content":"A solution to inheritance I\u0026rsquo;ve had some more thought about my programming language \u0026ldquo;Compost\u0026rdquo;. It has a solution to the old problems of object oriented programming, mainly class inheritance.\nOne of the problems class inheritance tries to solve is repeated code between classes, by creating a superclass that contains all the common methods, which subclasses inherit from it. But class inheritance comes with its own problems, and has gotten out of favor.\nI want to show an example of the classic \u0026ldquo;Shapes\u0026rdquo; library, often used to teach object oriented programming, but implemented in Compost. Just read through the code comments to see the possibilities.\nmod Vec2 class x: Float y: Float traits X: Float Y: Float Multiply: (factor: Float) -\u0026gt; Vec2 Add: (other: Vec2) -\u0026gt; Vec2 defs X: x Y: y Multiply: Vec2(X * factor, Y * factor) Add: Vec2(X + other.X, Y + other.Y) # Converts it to a Point. The Point trait is auto-declared by the Point module .Point: Point(X, Y) # For \u0026#39;public\u0026#39; fields I might short-hand this: mod Vec2 class x: Float traits X: Float defs X: x # To this: (the uppercase first character would auto-define all of the above) mod Vec2 class X: Float # A type of Vec2 with the same x and y. By defining the X and Y traits of Vec2, # it auto-defines all other Vec2 traits. You can use Vec2 and DiagonalVec2 instances # completely interchangeably because they have the same interfaces of traits. mod DiagonalVec2 class value: Float defs Vec2 X: value Y: value mod Point class x: Float y: Float traits X: Float Y: Float Translate: (offset: Vec2) -\u0026gt; Point defs X: x Y: y Translate: Point(x + offset.X, y + offset.Y) .Vec2: Vec2(x, y) # A function to create a \u0026#39;diagonal\u0026#39; point. This would work the same as # creating a whole DiagonalPoint class, since a class is just used a function. let DiagonalPoint(value: Float): DiagonalVec2(value).Point # Shape declares some traits but doesn\u0026#39;t define them. It works like an interface. mod Shape traits Center: Point Area: Float Perimeter: Float # Rectangle declares some traits but doesn\u0026#39;t define all of them. # It defines some of Shape\u0026#39;s traits using those declarations. mod Rectangle traits TopLeft: Point BottomRight: Point Size: Vec2 defs # Just to make clear we purposely haven\u0026#39;t defined this: ? means undefined. TopLeft: ? BottomRight: TopLeft.Translate(Size) Size: ? # We define Shape\u0026#39;s traits here. So if any class defines Rectangle\u0026#39;s traits, # Shape\u0026#39;s traits will also be defined for that class. Shape Center: Rectangle.TopLeft.Translate(Rectangle.Size.Multiply(0.5)) Area: Rectangle.Size.X * Rectangle.Size.Y Perimeter: (Rectangle.Size.X + Rectangle.Size.Y) * 2 # Declares but doesn\u0026#39;t define a Square.Size trait. Square is another interface. # It automatically defines the Rectangle.Size trait if the Square.Size trait is defined mod Square traits Size: Float defs Size: ? Rectangle: Size: Vec2(Square.Size, Square.Size) # One type of Square class, created by giving a top_left corner and a size. # This will define all of Square\u0026#39;s, all of Rectangle\u0026#39;s and all of Shape\u0026#39;s traits. mod TopLeftSquare class top_left: Point size: Float defs Square Size: size Rectangle TopLeft: top_left mod CenterSquare # Another type of Square class class center: Point size: Float defs Square Size: size Rectangle Center: center TopLeft: center.Translate(Rectangle.Size.Multiply(-0.5)) # A constant definition. let Pi: 3.14159265359 mod Circle class center: Point radius: Float defs Shape Center: center Area: Pi * radius * radius Perimeter: 2 * Pi * radius # A function definition that takes any shape. let AreaPlusPerimeter(shape: Shape.Area \u0026amp; Shape.Perimeter) shape.Area + shape.Perimeter # The main function let Main # Creates a square by specifying the top left corner and the size. # Then returns the Shape.Center trait of it. TopLeftSquare top_left: Point(x: 10, y: 10) size: 100 Center Defining functions or variables A thing I wasn\u0026rsquo;t sure about before is how to define functions and \u0026ldquo;variables\u0026rdquo;. I came to the conclusion that those two are really the same, since \u0026ldquo;variables\u0026rdquo; won\u0026rsquo;t change. They are really constants due to the fact that we\u0026rsquo;re a functional language. When \u0026ldquo;changing\u0026rdquo; a \u0026ldquo;variable\u0026rdquo; we\u0026rsquo;re just defining a new constant.\nIt was difficult to find a common keyword to define a function or a constant. I\u0026rsquo;ve come to the realisation that we don\u0026rsquo;t even need a keyword. Functions or constants will be defined like this:\nPi: 3.14 AddOne(x: Int): x + 1 LongerFunction(instance: MyInterface) instance Method1 Method2 Method1 Functions can be defined using a semicolon or by adding one or more indented lines below the name.\nIn the last function I showcase how method chaining is done. We simply put them on the next line.\nSo how do we define constants inside functions? Like this:\nAnotherFunction(instance: MyInterface) ConstantOne: instance.Method1 ConstantTwo: 3 ConstantThree instance Method3(ConstantTwo) Method1 ConstantOne.Method4(ConstantThree) To explain: it creates ConstantOne, which will contain the result of instance.Method1. ConstantTwo is simply 3. ConstantThree contains Method3(3) called on our instance, and then Method1 called on that. The function returns ConstantOne with Method4(ConstantThree) called on it.\nI just realized it might be difficult the notice the difference between chaining methods and defining functions/constants. I\u0026rsquo;ll have to find a solution for that. Maybe I should just use let to define anything. That would look like this:\nlet AnotherFunction(instance: MyInterface) let ConstantOne: instance.Method1 let ConstantTwo: 3 let ConstantThree instance Method3(ConstantTwo) Method1 ConstantOne.Method4(ConstantThree) ","permalink":"https://sytzez.com/blog/sketch-for-a-new-programming-language-2/","summary":"A solution to inheritance I\u0026rsquo;ve had some more thought about my programming language \u0026ldquo;Compost\u0026rdquo;. It has a solution to the old problems of object oriented programming, mainly class inheritance.\nOne of the problems class inheritance tries to solve is repeated code between classes, by creating a superclass that contains all the common methods, which subclasses inherit from it. But class inheritance comes with its own problems, and has gotten out of favor.","title":"Sketch for a new programming language: Part 2"},{"content":"I\u0026rsquo;m coming up with the spec of a new programming language. Working title: \u0026ldquo;Compost\u0026rdquo;. Don\u0026rsquo;t worry, it\u0026rsquo;s just for fun. For now this is just a first sketch of the ideas I had today. They are subject to change.\nThe main idea of the language is ultimate composability and recomposability. It forces the programmer to think in terms of the minimum amount of dependencies required for each part of the code, and makes each part as reusable as possible. It will also use \u0026lsquo;value\u0026rsquo; semantics and it will be highly encapsulated. It\u0026rsquo;s not possible to directly change or read any fields of an object except through traits. In fact it will be impossible (and irrelevant) to know what class and object is, types will be defined solely on the basis of traits.\nClasses will not be allowed to have any raw fields (such as ints, bools, floats), instead their dependencies are defined on the basis of traits, making them completely interchangeable.\nLet\u0026rsquo;s start off with some example code:\nmod Cat # Defines a module, an interface and a class, all called \u0026#39;Cat\u0026#39;. It also defines a trait called \u0026#39;Cat\u0026#39; with type () -\u0026gt; Cat, to transform other things into a Cat class # Dependencies of the Cat class name: String # String is an interface birthDate: Date # Date is also an interface traits # The traits making up the Cat interface Name: String # Trait Cat.Name has no arguments and returns a String interface Age: (now: Date) -\u0026gt; Timespan # Cat.Age takes a Date interface and returns a Timespan interface defs # The trait definitions for the Cat class Name # Defines the Cat.Name trait for Cat name # Simply return Cat.name Age # Defines the Cat.Age trait for Cat Date.Diff: birthDate, now # Call function Diff from module Date. Use Cat.birthDate and the argument from the Age trait String # Define the standard String trait for a Cat, meaning the Cat can be transformed into a String Name + \u0026#34; \u0026#34; + Age.String # Return the Cat.name and Cat.Age converted into a String # Because this String definition doesn\u0026#39;t use any dependencies of Cat, it can be implemented automatically for other classes implementing the Cat interface I could shorten the defs to:\ndefs Name: name Age: Date.Diff(birthDate, now) String: name + \u0026#34; \u0026#34; + Age.String A lot of stuff is implied from the context by the code. The full code would be:\nmod Cat traits # Some traits on module Cat Name: String Age: (now: Date) -\u0026gt; Timespan interface Cat # Interface Cat, consisting of traits Cat.Name Cat.Age String class Cat implements Cat # Class Cat implementing the interface. Lists dependencies. name: String birthDate: Date def Cat.Name for Cat Cat.name def Cat.Age for Cat Date.Diff: Cat.birthDate, Age.now def String for Cat Cat.Name + \u0026#34; \u0026#34; + Cat.Age.String This exports a lot of things that can be used outside of the mod:\nThe Cat.Name and Cat.Age traits. The Cat interface, made out of the Cat.Name, Cat.Age and String traits. One function: Cat, which takes a name and a birthDate and returns an object implementing Cat. Ergo, the Cat constructor. The Cat trait, which can be defined for other classes that can be converted into a Cat or return a Cat. Type: () -\u0026gt; Cat. No other functions. We haven\u0026rsquo;t declared any, we could. The Cat class, which can be used only to create subclasses inheriting its trait definitions. It can\u0026rsquo;t be a type. Let\u0026rsquo;s define another class in this context:\nmod Human class name: String cat: Cat # This dependency needs to implement the Cat interface. It doesn\u0026#39;t have to be the Cat class. birthDate: Date traits Name: String Age: (now: Date) -\u0026gt; Timespan defs Name: name Age: Date.Diff: birthDate, now Cat: cat # We define the trait Cat from the Cat module for Human String: Name + \u0026#34; \u0026#34; + Age.String + \u0026#34; owning cat: \u0026#34; + Cat.String # This calls the Cat trait on our Human, and calls the String trait on the resulting Cat. As you might have guessed, we now have some very similar traits:\nCat.Name and Human.Name Cat.Age and Human.Age This might be a good thing. If you think of a name as an id, it\u0026rsquo;s good to keep different types of them. A Cat and Human might have the same name, but they refer to different entities.\nHowever, you might also say a name is a name, and it shouldn\u0026rsquo;t matter what it\u0026rsquo;s naming. In that case, you can extract the Name trait from both:\ntrait Name: String And then you don\u0026rsquo;t have to declare the trait on the Human and the Cat, you only have to define its implementation. It will still be part of the Cat and Human interfaces if you define it within their modules.\nThere are two things I had imagined which we haven\u0026rsquo;t covered yet, declaring functions and structs.\nA function may take some arguments, and MUST return at least one value. It CAN NOT have side effects. So we\u0026rsquo;re fully functional ;). It can return the same type as one of its arguments though, in which case we can use the function to replace the original value.\nfn ConstantValue: () -\u0026gt; Int 42 fn ConstantValue: Int # Shorthand 42 fn AddOne: (x: Int) -\u0026gt; Int x + 1 fn AddOne: \u0026amp;Int # Shorthand for the above AddOne.0 + 1 fn AddOne: (x: Int) -\u0026gt; x # Signifies we could be replacing x x + 1 fn AddOne: (\u0026amp;x: Int) # Shorthand for the above x + 1 The last method can be used like this: let x: ConstantValue AddOne!: x # The exclamation mark lets us know we\u0026#39;re changing x let y: AddOne(x) # Doesn\u0026#39;t change x\nA trait can also have a \u0026amp; type, in which we can use it to \u0026ldquo;update\u0026rdquo; the class, i.e. override it, or override part of it. I still have to figure out how this will work exactly, but I\u0026rsquo;d like to be able to do Cat.Rename!: \u0026quot;Bobo\u0026quot;\nLastly, instead of classes, we can define structs. While class dependencies can only be things implementing traits, struct dependencies can only be core values.\nmod I64 implements Int struct x: i64 defs Op.Add: I64(x + Op.Add.right.x) # Creates a new struct of itself. We can access the private memory of `right` because it\u0026#39;s the same struct type. # etc. Like with a class module, we can define traits for the struct, and implement an interface.\nOp.Add Is defined like this:\nmod Op trait Add: (right: Self) -\u0026gt; Self The Self type obviously refers to whatever type the trait is being defined on.\nThis is it for now, it\u0026rsquo;s just a sketch. I might post more about it in the future.\n","permalink":"https://sytzez.com/blog/sketch-for-a-new-programming-language/","summary":"I\u0026rsquo;m coming up with the spec of a new programming language. Working title: \u0026ldquo;Compost\u0026rdquo;. Don\u0026rsquo;t worry, it\u0026rsquo;s just for fun. For now this is just a first sketch of the ideas I had today. They are subject to change.\nThe main idea of the language is ultimate composability and recomposability. It forces the programmer to think in terms of the minimum amount of dependencies required for each part of the code, and makes each part as reusable as possible.","title":"Sketch for a new programming language"},{"content":"In this blog I will show how I composed the baroque style piece \u0026lsquo;Ouverture\u0026rsquo; starting with simple, strict two-voice counterpoint, and then embellishing that into a fuller baroque style.\nMost of the material is derived from a single melody which is varied using augmentation, fragmentation, inversion and embellishment.\nIn the sheet music for this blog I\u0026rsquo;ve added numbers in between the two staves indicating the intervals between the top and the bottom voice. This is intended to make the two-voice counterpoint clearer. 6ths and 3rds are good, 8ths and 5ths are ok. Consecutive 8ths and 5ths are forbidden. Dissonant 2nds, 4ths and 7ths need to be on weak beats, unless they have a good \u0026lsquo;reason\u0026rsquo; not to be.\nGood reasons for these dissonant intervals are the rules for \u0026rsquo;nonchord tones\u0026rsquo;, which I\u0026rsquo;ve also included in the score as abbreviations. There\u0026rsquo;s a visual explanation of these rules here, and also on wikipedia. Another reason dissonant intervals could be justified is if they sound like a consonant interval. For example, an augmented 2nd sounds like a minor 3rd.\nHere\u0026rsquo;s a list of the abbreviations that are used:\nAbbreviation Meaning Ret. Retardation. A voice moves up later than expected. This has to be on an accented beat. Sus. Suspension. A voice moves down later than expected. Also on an accented beat. NT Neighboring tone. A voice moves up or down and then back. PT Passing tone. A voice uses an intermediary step to go to another tone. \u0026gt;PT Accented passing tone. The same but on an accented beat. Ant. Anticipation. A voice moves up or down sooner than expected. Emb. Embellishment. Multiple notes are added. Section A First phrase The piece starts with the main melody in the top voice. The melody is played simultaneously an octave below with double note values, meaning at half the speed, also known as \u0026lsquo;augmented\u0026rsquo;. The phrase closes with a cadence in the home key of F minor.\nYour browser does not support HTML5 audio. In the next version, the eighth note run is sped up into a sixteenth note run. Non-chord tones and embellishments are added that add dissonance.\nYour browser does not support HTML5 audio. In the final version, there are also some middle voices. (You can listen to the final piece at the bottom of the page)\nSecond phrase The second phrase consists of two similar sequences, both moving downwards by step. The bass uses the first four notes of the main melody, with the third note elongated. The top voice also uses a variation of the beginning of the melody.\nAfter the first sequence, there is a cadence in A flat major, the relative major of the home key. After the second sequence there is a half-cadence in C minor, which is the dominant key of the home key.\nThe harmony lingers on the dominant of C minor for a bit before concluding with a full cadence in C minor, with a picardy third at the end.\nYour browser does not support HTML5 audio. In the next iteration, embellishments are added. The three note run turns into a four note run in the second sequence. The amount of runs increases when the music approaches the final cadence.\nYour browser does not support HTML5 audio. In the final iteration, as in the first section, more middle voices are added to enrich the harmony.\nSection B Third phrase This phrase, in the dominant key of C minor, starts with an inversion of the melody in both the top and the bottom voice. The bottom voice, as in the first phrase, contains an augmented version of that melody. In bar 3, an other voice enters below the previous bottom voice. The phrase ends with a half cadence in G minor.\nYour browser does not support HTML5 audio. Suspensions and other nonchord tones are added in the next version:\nYour browser does not support HTML5 audio. In the final version, another voice is added. Strange chords such as augmented chords are formed.\nFourth phrase After the previous phrase ended on the dominant of G minor, this phrase unexpectedly starts in B flat major, the relative major of G minor and also the subdominant of the home key.\nLike the second phrase, it used shortened variations of the melody to build a sequence. This time the melody is inverted, and the sequence moves up instead of down. The top voice has starts out with a fragment of the melody but then continues freely. At the end of the phrase there is a half cadence in the home key of F minor.\nYour browser does not support HTML5 audio. In the next iteration, again, embellishments are added. More and more sixteenth notes are added as the melody rises and the half cadence is approached. The last two bars before the half cadence contain a long downwards run in the top voice.\nYour browser does not support HTML5 audio. Fifth phrase After the last phrase ended on a half cadence in F minor, the home key, the bass moves down stepwise from the 5th to the 1st degree.\nOn top of the bass, two voices imitate each other, playing variations of the beginning of the main melody, both rising. Then, there\u0026rsquo;s a final cadence to conclude the piece.\nYour browser does not support HTML5 audio. In the final version, more voices are added.\nResult Listen to the complete piece.\n","permalink":"https://sytzez.com/blog/composing-ouverture/","summary":"In this blog I will show how I composed the baroque style piece \u0026lsquo;Ouverture\u0026rsquo; starting with simple, strict two-voice counterpoint, and then embellishing that into a fuller baroque style.\nMost of the material is derived from a single melody which is varied using augmentation, fragmentation, inversion and embellishment.\nIn the sheet music for this blog I\u0026rsquo;ve added numbers in between the two staves indicating the intervals between the top and the bottom voice.","title":"Composing 'Ouverture'"},{"content":"In this blog I will explain how to easily add sheet music to your site, such as the example below. Note how the division of the bars adjusts when you shrink your screen width, it\u0026rsquo;s truly responsive!\nIntroduction When writing blogs about music, you might want to render snippets of music notation in between paragraphs of text. I will show you how to easily embed MusicXML content within your Hugo posts.\nMusicXML is the most common format of exchanging musical notation information between different programs. Whatever musical notation software you use, chances are very high they have an \u0026ldquo;export to MusicXML\u0026rdquo; functionality. Examples include Sibelius, Finale, noteflight and many others. If you want to use a free, fully functional program to create or edit musical notation, I recommend downloading MuseScore.\nTo render your MusicXML files as notation on your Hugo site, we will be using a javascript library called OpenSheetMusicDisplay. It reads MusicXML and renders it as an SVG using the widely used VexFlow library. It also supports guitar tabs.\n1. Add scripts to the header To start, add the following scripts to your header:\n\u0026lt;script id=\u0026#34;osmd-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/opensheetmusicdisplay@1.5.7/build/opensheetmusicdisplay.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; (function () { // Wait for the DOM to render and the osmd script to load. document.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, loadOsmd) document.getElementById(\u0026#39;osmd-script\u0026#39;).addEventListener(\u0026#39;load\u0026#39;, loadOsmd) var wait = true function loadOsmd() { // This is just to make sure both load events have fired before we do anything. if (wait) { wait = false return } // Iterate through all elements with class \u0026#39;osmd-container\u0026#39;. document.querySelectorAll(\u0026#39;.osmd-container\u0026#39;).forEach(function (container) { // Create an OpenSheetMusicDisplay object for the container. var osmd = new opensheetmusicdisplay.OpenSheetMusicDisplay(container, { // Use minimal spacing and hide the title and instrument names. drawingParameters: \u0026#39;compacttight\u0026#39; }) // Load the MusicXML file. osmd.load(container.dataset.musicXmlSrc) .then(function () { // Render the notation inside the container. osmd.render() }) }) } })(); \u0026lt;/script\u0026gt; It loads the OpenSheetMusicDisplay script asynchronously. After that script has been loaded and the DOM has finished rendering, it goes through all elements with the osmd-container class. For each of those containers, it creates a new OpenSheetMusicDisplay object and renders the MusicXML content inside it.\n2. Create the music-xml shortcode To populate a page with osmd-container classes we can create a Hugo shortcode by adding a file called layouts/shortcodes/music-xml.html with the following contents:\n\u0026lt;div class=\u0026#34;osmd-container\u0026#34; data-music-xml-src=\u0026#34;{{ .Get 0 }}\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; It simply renders an empty div with the osmd-container class and a data-music-xml-src attribute containing the MusicXML url which is provided to the shortcode as an argument. The header script will convert this into musical notation.\n3. Use the shortcode inside your content The shortcode can be used from within any Hugo content or layout file:\n{{\u0026lt; music-xml \u0026#34;/music-xml/your-music-xml.mxl\u0026#34; \u0026gt;}} Simply provide the local or global path to a MusicXML file.\nNote that this doesn\u0026rsquo;t produce any audio by itself. I like to add an \u0026lt;audio\u0026gt; tag with an mp3 file below the notation to provide playback functionality.\n","permalink":"https://sytzez.com/blog/responsive-sheet-music-in-hugo-using-music-xml/","summary":"In this blog I will explain how to easily add sheet music to your site, such as the example below. Note how the division of the bars adjusts when you shrink your screen width, it\u0026rsquo;s truly responsive!\nIntroduction When writing blogs about music, you might want to render snippets of music notation in between paragraphs of text. I will show you how to easily embed MusicXML content within your Hugo posts.","title":"Responsive sheet music in Hugo using MusicXML"},{"content":"The two skills of solfege Solfege, in its fullest sense, is the art of translating musical ideas from one representation into another. It teaches the skill of, on the one hand, recognising music in all its forms and, on the other hand, reproducing music in another form.\ngraph RL A[Music] subgraph solfege B[Recognition] C[Reproduction] end A---B C---A Music\u0026rsquo;s primary medium is sound. Another common medium is music notation, which is written using special musical symbols known as \u0026rsquo;notes\u0026rsquo;. Besides being represented by symbols, music can be described in written or spoken language. The availability of musicological terminology can make verbal descriptions as precise or even preciser than musical notation.\nSolfege teaches the ability of understanding each of these three forms, as well as being able to accurately reproduce the same musical idea in another form.\nThe three representations A musical element is the smallest building block of music such as a pitch, an interval or a note value. Each musical element can be expressed in the three different representations mentioned before. If you are able to recognise and reproduce each representation, you have mastered the solfege of that specific musical element. The three reperesentations of a musical element are its sound, its name and its notation.\ngraph LR subgraph Musical element A[Name]---B[Sound] B---C[Notation] C---A end For example: the pitch \u0026lsquo;C4\u0026rsquo; (C natural in the fourth octave) has an identifiable sound quality, a certain frequency. While the timbre, dynamic or articulation of the sound might vary, the identifiable part is its frequency (assuming a certain tuning). Regardless of the tuning, it has an identifiable quality in the context of other pitches in the same tuning: C4 is always a whole tone below D4. Besides its sound quality, C4 also has a position on the musical staff, relative to the clef. Its sound is its sound, its name is \u0026lsquo;C4\u0026rsquo;, its notation is the position on the staff.\nAnother example is the interval of a major third (M3). While the M3 doesn\u0026rsquo;t have a specific pitch, its quality is identifiable when heard. Each dyad of notes that is a major third apart has that sound quality. It is also recognisable in notation, by the vertical distance between two notes.\nYou could argue that a major third (M3) is not identifiable purely by its sound, because an augmented second (A2) has the same sound quality. When the interval is isolated this is true, but in a broader musical context (assuming there is tonal harmony), there is an objective difference between a M3 and a A2, related to the melodic outline it\u0026rsquo;s used in, or the functional harmonic progression the chord with the interval is part of. making it possible to recognise the different between a M3 or a A2 purely by listening. And that is definitely part of solfege.\nThe same three representation can be derived from rhythmic note values, chords, dynamics, articulation, tempo indications, tonalities, modulations, time signatures and other musical elements.\nThe six modes We\u0026rsquo;ve determined that solfege requires both recognising and reproducing musical representations. We\u0026rsquo;ve also determined that there are three main representations of a musical element.\nFor each musical element, we can \u0026lsquo;do\u0026rsquo; solfege in a number of different ways by varying the representation we recognise and the representation we reproduce. Since there are three representations, there must be six (two times three) ways of doing solfege. I\u0026rsquo;ll call them modes of solfege.\ngraph TD A[Sound]--\u003e|Sound to Language|B[Language] B--\u003e|Language to Notation|C[Notation] C--\u003e|Notation to Sound|A A--\u003e|Sound to Notation|C B--\u003e|Language to Sound|A C--\u003e|Notation to Language|B To master the solfege of a musical element means to master each of these six modes: Sound to language, language to notation, notation to sound, sound to notation, language to sound and notation to language. Each of them can be practised separately.\nTo give example of one of these modes: \u0026lsquo;sound to name\u0026rsquo; means listening to a sound and being able to call the element by its name, without using any notation. \u0026lsquo;Notation to sound\u0026rsquo; means being able to read notes and hear in your head or physically reproduce the sound accurately. With some imagination you can easily imagine the other modes.\nOnce you\u0026rsquo;re able to succesfully perform each of these six modes for a specific musical element, the three representations will become a unified trinity in your mind. You\u0026rsquo;ll be able to understand and utilize the musical element to its full extent.\n","permalink":"https://sytzez.com/blog/modes-of-solfege/","summary":"The two skills of solfege Solfege, in its fullest sense, is the art of translating musical ideas from one representation into another. It teaches the skill of, on the one hand, recognising music in all its forms and, on the other hand, reproducing music in another form.\ngraph RL A[Music] subgraph solfege B[Recognition] C[Reproduction] end A---B C---A Music\u0026rsquo;s primary medium is sound. Another common medium is music notation, which is written using special musical symbols known as \u0026rsquo;notes\u0026rsquo;.","title":"Modes of Solfege"},{"content":"I\u0026rsquo;ve been thinking a while about starting a blog to get various ideas out there in the world, whether they be about programming, music or other things. Getting content out there on \u0026rsquo;the web\u0026rsquo; might help me promote the things I\u0026rsquo;m doing, attract likeminded people or people that can help me in my endeavours, and it can help me hold myself accountable to the ideas I get and the goals I set for myself. Possibly it will do none of those things, but it will still help me think \u0026lsquo;out loud\u0026rsquo; and allow me to practise putting thoughts into words.\nCreating any content whatsoever will also train my content muscle and make me a better content producer, so for now the quality doesn\u0026rsquo;t really matter. I\u0026rsquo;ll just try creating as many blogs as possible. (warning to self: that sounds like a goal and accountability). Creating something daily or even weekly might be a bar too high, with the little amount of free time I have these days. I won\u0026rsquo;t set any goals for now, let\u0026rsquo;s see where it goes.\nThe thing stopping me from doing this lately was simply not being able to choose a platform to blog on. I wanted something that won\u0026rsquo;t cost me too much time and effort, so building my own site in Laravel or Ruby on Rails was out of the question. I\u0026rsquo;ve tried Wordpress and other more modern content managing systems like Wix, but those just didn\u0026rsquo;t feel right. Then I came across the Hugo templating system, driven by the Go programming language. I was amazed at first by the speed of it. It was able to render over 18000 pages in under a minute at the company I\u0026rsquo;m working at. When I looked more into it, I really liked the file structure it forces you to use, so you don\u0026rsquo;t have to make too many decisions yourself, and the fact that it deals with assets such as CSS and JS/TS for you, and that it allows you to just write pages using Markdown. I was already using Markdown for personal and work related notes, using a pretty cool tool called \u0026lsquo;Obsidian\u0026rsquo;, so it wouldn\u0026rsquo;t be a huge adaptation for me to start blogging in Markdown. I also like that is just generates a static site that I can plop onto any hosting platform or even just on Github sites. I also like being able to just Git versioning and not having to rely on some website to edit my content.\nFor now I\u0026rsquo;m just using an existing theme to get familiar with Hugo templating, but I\u0026rsquo;ll probably create my own simple theme at some point. (Uh oh, more accountability). Anyway that\u0026rsquo;s enough for now.\nSome ideas of future blog posts:\nLaying out my ideas of classical music composition. (I already have a draft ready) Analysing some pieces using those ideas. Implementing a shortcode to display music notation in Hugo. (using Vexflow) Talking about the music app I\u0026rsquo;m working on. Discovery and comparison of different existing apps similar to the app I\u0026rsquo;m building (and how mine\u0026rsquo;s better). Programming related stuff, especially related to Rust. Who knows what else\u0026hellip; ","permalink":"https://sytzez.com/blog/initial-post/","summary":"I\u0026rsquo;ve been thinking a while about starting a blog to get various ideas out there in the world, whether they be about programming, music or other things. Getting content out there on \u0026rsquo;the web\u0026rsquo; might help me promote the things I\u0026rsquo;m doing, attract likeminded people or people that can help me in my endeavours, and it can help me hold myself accountable to the ideas I get and the goals I set for myself.","title":"Initial post"}]